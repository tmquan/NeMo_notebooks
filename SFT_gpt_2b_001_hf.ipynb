{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/NVIDIA/NeMo\n",
    "# https://github.com/NVIDIA/NeMo-Megatron-Launcher\n",
    "# https://github.com/NVIDIA/Megatron-LM\n",
    "\n",
    "# conda create -n py310 python=3.10\n",
    "# # sudo apt update\n",
    "# # sudo apt install libsndfile1 ffmpeg\n",
    "# conda activate py310\n",
    "\n",
    "# pip install -U Cython\n",
    "# pip install -U python-dotenv\n",
    "\n",
    "# pip install -U nemo_toolkit['all']\n",
    "\n",
    "# pip install -U -v --no-build-isolation --disable-pip-version-check --no-cache-dir --config-settings \\\n",
    "# \"--build-option=--cpp_ext --cuda_ext --fast_layer_norm --distributed_adam --deprecated_fused_adam\" git+https://github.com/NVIDIA/apex.git@c07a4cf67102b9cd3f97d1ba36690f985bae4227\n",
    "\n",
    "# pip install -U git+https://github.com/NVIDIA/NeMo-Megatron-Launcher.git\n",
    "# pip install -U git+https://github.com/NVIDIA/Megatron-LM.git\n",
    "# pip install -U git+https://github.com/NVIDIA/NeMo.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'databricks-dolly-15k'...\n",
      "remote: Enumerating objects: 36, done.\u001b[K\n",
      "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
      "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
      "remote: Total 36 (delta 7), reused 36 (delta 7), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (36/36), 8.39 KiB | 1.40 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://huggingface.co/datasets/databricks/databricks-dolly-15k\n",
    "# ! wget https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13M\tdatabricks-dolly-15k/databricks-dolly-15k.jsonl\n"
     ]
    }
   ],
   "source": [
    "! du -sh databricks-dolly-15k/databricks-dolly-15k.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"instruction\": \"When did Virgin Australia start operating?\", \"context\": \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\", \"response\": \"Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.\", \"category\": \"closed_qa\"}\n",
      "{\"instruction\": \"Which is a species of fish? Tope or Rope\", \"context\": \"\", \"response\": \"Tope\", \"category\": \"classification\"}\n",
      "{\"instruction\": \"Why can camels survive for long without water?\", \"context\": \"\", \"response\": \"Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.\", \"category\": \"open_qa\"}\n",
      "{\"instruction\": \"Alice's parents have three daughters: Amy, Jessy, and what\\u2019s the name of the third daughter?\", \"context\": \"\", \"response\": \"The name of the third daughter is Alice\", \"category\": \"open_qa\"}\n",
      "{\"instruction\": \"When was Tomoaki Komorida born?\", \"context\": \"Komorida was born in Kumamoto Prefecture on July 10, 1981. After graduating from high school, he joined the J1 League club Avispa Fukuoka in 2000. Although he debuted as a midfielder in 2001, he did not play much and the club was relegated to the J2 League at the end of the 2001 season. In 2002, he moved to the J2 club Oita Trinita. He became a regular player as a defensive midfielder and the club won the championship in 2002 and was promoted in 2003. He played many matches until 2005. In September 2005, he moved to the J2 club Montedio Yamagata. In 2006, he moved to the J2 club Vissel Kobe. Although he became a regular player as a defensive midfielder, his gradually was played less during the summer. In 2007, he moved to the Japan Football League club Rosso Kumamoto (later Roasso Kumamoto) based in his local region. He played as a regular player and the club was promoted to J2 in 2008. Although he did not play as much, he still played in many matches. In 2010, he moved to Indonesia and joined Persela Lamongan. In July 2010, he returned to Japan and joined the J2 club Giravanz Kitakyushu. He played often as a defensive midfielder and center back until 2012 when he retired.\", \"response\": \"Tomoaki Komorida was born on July 10,1981.\", \"category\": \"closed_qa\"}\n"
     ]
    }
   ],
   "source": [
    "! head -5 databricks-dolly-15k/databricks-dolly-15k.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data to jsonl format...\n",
      "Data was successfully preprocessed and saved by databricks-dolly-15k/databricks-dolly-15k-output.jsonl .\n"
     ]
    }
   ],
   "source": [
    "# ! python NeMo-Megatron-Launcher/launcher_scripts/nemo_launcher/collections/dataprep_scripts/dolly_dataprep/preprocess.py \\\n",
    "#     --input databricks-dolly-15k/databricks-dolly-15k.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\\n\\nWhen did Virgin Australia start operating?\", \"output\": \"Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.\", \"category\": \"closed_qa\"}\n",
      "{\"input\": \"Which is a species of fish? Tope or Rope\", \"output\": \"Tope\", \"category\": \"classification\"}\n",
      "{\"input\": \"Why can camels survive for long without water?\", \"output\": \"Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.\", \"category\": \"open_qa\"}\n",
      "{\"input\": \"Alice's parents have three daughters: Amy, Jessy, and what\\u2019s the name of the third daughter?\", \"output\": \"The name of the third daughter is Alice\", \"category\": \"open_qa\"}\n",
      "{\"input\": \"Komorida was born in Kumamoto Prefecture on July 10, 1981. After graduating from high school, he joined the J1 League club Avispa Fukuoka in 2000. Although he debuted as a midfielder in 2001, he did not play much and the club was relegated to the J2 League at the end of the 2001 season. In 2002, he moved to the J2 club Oita Trinita. He became a regular player as a defensive midfielder and the club won the championship in 2002 and was promoted in 2003. He played many matches until 2005. In September 2005, he moved to the J2 club Montedio Yamagata. In 2006, he moved to the J2 club Vissel Kobe. Although he became a regular player as a defensive midfielder, his gradually was played less during the summer. In 2007, he moved to the Japan Football League club Rosso Kumamoto (later Roasso Kumamoto) based in his local region. He played as a regular player and the club was promoted to J2 in 2008. Although he did not play as much, he still played in many matches. In 2010, he moved to Indonesia and joined Persela Lamongan. In July 2010, he returned to Japan and joined the J2 club Giravanz Kitakyushu. He played often as a defensive midfielder and center back until 2012 when he retired.\\n\\nWhen was Tomoaki Komorida born?\", \"output\": \"Tomoaki Komorida was born on July 10,1981.\", \"category\": \"closed_qa\"}\n"
     ]
    }
   ],
   "source": [
    "! head -5 databricks-dolly-15k/databricks-dolly-15k-output.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # HF_TOKEN\n",
    "HF_TOKEN = os.environ[\"HF_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"gpt-2b-001-hf\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b3abef794d47d6b7a7cbeaa1322dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1e793c539c494284e4f6da1373cd5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GPT-2B-001_bf16_tp1.nemo:   0%|          | 0.00/9.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3b61fb71d1403c9c229f2e010d234c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ba1d74a2f24d0d8c853391211a7c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/quantm/NeMo_notebooks/gpt-2b-001-hf'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_download(\n",
    "    repo_id=\"nvidia/GPT-2B-001\", \n",
    "    local_dir=\"gpt-2b-001-hf\", \n",
    "    local_dir_use_symlinks=False, \n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile split_train_val.py\n",
    "import json\n",
    "import random\n",
    "\n",
    "data_dir = \"databricks-dolly-15k\"\n",
    "input_file = f\"{data_dir}/databricks-dolly-15k-output.jsonl\"\n",
    "training_output_file = f\"{data_dir}/training.jsonl\"\n",
    "validation_output_file = f\"{data_dir}/validation.jsonl\"\n",
    "test_output_file = f\"{data_dir}/test.jsonl\"\n",
    "\n",
    "# Specify the proportion of data for training and validation\n",
    "train_proportion = 0.80\n",
    "validation_proportion = 0.15\n",
    "test_proportion = 0.05\n",
    "\n",
    "# Read the JSONL file and shuffle the JSON objects\n",
    "with open(input_file, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    random.shuffle(lines)\n",
    "\n",
    "# Calculate split indices\n",
    "total_lines = len(lines)\n",
    "train_index = int(total_lines * train_proportion)\n",
    "val_index = int(total_lines * validation_proportion)\n",
    "\n",
    "# Distribute JSON objects into training and validation sets\n",
    "train_data = lines[:train_index]\n",
    "validation_data = lines[train_index:train_index+val_index]\n",
    "test_data = lines[train_index+val_index:]\n",
    "\n",
    "# # Write JSON objects to training file\n",
    "# with open(training_output_file, \"w\") as f:\n",
    "#     for line in train_data:\n",
    "#         f.write(line.strip() + \"\\n\")\n",
    "\n",
    "# # Write JSON objects to validation file\n",
    "# with open(validation_output_file, \"w\") as f:\n",
    "#     for line in validation_data:\n",
    "#         f.write(line.strip() + \"\\n\")\n",
    "\n",
    "# # Write JSON objects to training file\n",
    "# with open(test_output_file, \"w\") as f:\n",
    "#     for line in test_data:\n",
    "#         f.write(line.strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python split_train_val.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dotenv module\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Define the variables\n",
    "data_dir = \"databricks-dolly-15k\"\n",
    "TRAIN = f\"[{data_dir}/training.jsonl]\"\n",
    "VALID = f\"[{data_dir}/validation.jsonl]\"\n",
    "TEST = f\"[{data_dir}/test.jsonl]\"\n",
    "VALID_NAMES = f\"[databricks-dolly-15k]\"\n",
    "\n",
    "CONCAT_SAMPLING_PROBS=\"[1.0]\"\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# Create a dictionary with the variables\n",
    "variables = {\n",
    "    \"TRAIN\": TRAIN,\n",
    "    \"VALID\": VALID,\n",
    "    \"TEST\": TEST,\n",
    "    \"VALID_NAMES\": VALID_NAMES,\n",
    "    \"CONCAT_SAMPLING_PROBS\": CONCAT_SAMPLING_PROBS, \n",
    "    \"TP_SIZE\": TP_SIZE, \n",
    "    \"PP_SIZE\": PP_SIZE, \n",
    "}\n",
    "\n",
    "# Write the variables to the .env file\n",
    "with open('.env', 'a') as f:\n",
    "    for key, value in variables.items():\n",
    "        f.write(f\"{key}={value}\\n\")\n",
    "\n",
    "# Read back the values from the .env file to verify\n",
    "config = dotenv_values(\".env\")\n",
    "# display(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-02-01 15:30:42 nemo_logging:349] /home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "[NeMo W 2024-02-01 15:30:42 nemo_logging:349] /home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/lightning_fabric/connector.py:554: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "      rank_zero_warn(\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2024-02-01 15:30:42 exp_manager:694] Exp_manager is logging to results, but it already exists.\n",
      "[NeMo W 2024-02-01 15:30:42 exp_manager:616] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :results/checkpoints. Training from scratch.\n",
      "[NeMo W 2024-02-01 15:30:42 exp_manager:952] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 50. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-02-01 15:30:42 megatron_gpt_finetuning:56] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-02-01 15:30:42 megatron_gpt_finetuning:57] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: bf16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 50\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 0.1\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: results\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_loss\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: false\n",
      "        save_best_model: true\n",
      "      create_early_stopping_callback: true\n",
      "      early_stopping_callback_params:\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        min_delta: 0.001\n",
      "        patience: 10\n",
      "        verbose: true\n",
      "        strict: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 128\n",
      "      micro_batch_size: 4\n",
      "      restore_from_path: gpt-2b-001-hf/GPT-2B-001_bf16_tp1.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: false\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: true\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      peft:\n",
      "        peft_scheme: adapter\n",
      "        restore_from_path: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "        selective_tuning:\n",
      "          tunable_base_param_names:\n",
      "          - self_attention\n",
      "          - word_embeddings\n",
      "      data:\n",
      "        train_ds:\n",
      "          file_names:\n",
      "          - databricks-dolly-15k/training.jsonl\n",
      "          global_batch_size: 128\n",
      "          micro_batch_size: 1\n",
      "          shuffle: true\n",
      "          num_workers: 0\n",
      "          memmap_workers: 2\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          concat_sampling_probabilities:\n",
      "          - 1.0\n",
      "          label_key: output\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          truncation_field: input\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: '{input} {output}'\n",
      "          truncation_method: right\n",
      "        validation_ds:\n",
      "          file_names:\n",
      "          - databricks-dolly-15k/validation.jsonl\n",
      "          names:\n",
      "          - databricks-dolly-15k\n",
      "          global_batch_size: 128\n",
      "          micro_batch_size: 1\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "        test_ds:\n",
      "          file_names:\n",
      "          - databricks-dolly-15k/test.jsonl\n",
      "          names: null\n",
      "          global_batch_size: 256\n",
      "          micro_batch_size: 1\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 5.0e-06\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 50\n",
      "          min_lr: 0.0\n",
      "          constant_steps: 0\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "    \n",
      "[NeMo I 2024-02-01 15:30:42 exp_manager:396] Experiments will be logged at results\n",
      "[NeMo I 2024-02-01 15:30:42 exp_manager:842] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-02-01 15:30:50 megatron_gpt_model:238] megatron-core was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/NeMo#megatron-gpt.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "24-02-01 15:30:50 - PID:26956 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 128\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:437] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 15:30:50 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-02-01 15:30:50 megatron_init:241] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-02-01 15:30:50 megatron_init:247] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-02-01 15:30:50 megatron_init:252] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-02-01 15:30:50 megatron_init:255] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-02-01 15:30:50 megatron_init:272] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-02-01 15:30:50 megatron_init:275] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-02-01 15:30:50 megatron_init:276] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-02-01 15:30:50 megatron_init:287] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-02-01 15:30:50 megatron_init:288] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-02-01 15:30:50 megatron_init:298] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-02-01 15:30:50 megatron_init:302] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-02-01 15:30:50 megatron_init:303] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-02-01 15:30:50 megatron_init:317] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-02-01 15:30:50 megatron_init:329] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-02-01 15:30:50 megatron_init:335] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-02-01 15:30:50 megatron_init:336] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-02-01 15:30:50 megatron_init:337] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-02-01 15:30:50 megatron_init:338] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2024-02-01 15:30:50 tokenizer_utils:191] Getting SentencePiece with model: /tmp/tmp2wa5l_cm/2053796188904e679f7e2754a2a1f280_mt_nlg_plus_multilingual_ja_zh_the_stack_frac_015_256k.model\n",
      "[NeMo I 2024-02-01 15:30:50 megatron_base_model:520] Padded vocab_size: 256000, original vocab_size: 256000, dummy tokens: 0.\n",
      "[NeMo I 2024-02-01 15:30:57 nlp_overrides:1093] Model MegatronGPTSFTModel was successfully restored from /home/quantm/NeMo_notebooks/gpt-2b-001-hf/GPT-2B-001_bf16_tp1.nemo.\n",
      "[NeMo I 2024-02-01 15:30:57 megatron_gpt_finetuning:72] Adding adapter weights to the model for PEFT\n",
      "[NeMo I 2024-02-01 15:30:57 nlp_adapter_mixins:184] Before adding PEFT params:\n",
      "      | Name  | Type          | Params\n",
      "    ----------------------------------------\n",
      "    0 | model | Float16Module | 2.3 B \n",
      "    ----------------------------------------\n",
      "    0         Trainable params\n",
      "    2.3 B     Non-trainable params\n",
      "    2.3 B     Total params\n",
      "    9,014.362 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-02-01 15:30:57 nemo_logging:349] /home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-02-01 15:31:01 nlp_adapter_mixins:197] After adding PEFT params:\n",
      "      | Name  | Type          | Params\n",
      "    ----------------------------------------\n",
      "    0 | model | Float16Module | 2.3 B \n",
      "    ----------------------------------------\n",
      "    6.5 M     Trainable params\n",
      "    2.3 B     Non-trainable params\n",
      "    2.3 B     Total params\n",
      "    9,040.314 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-02-01 15:31:01 nemo_logging:349] /home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:153: UserWarning: The `batch_idx` argument in `MegatronGPTSFTModel.on_train_batch_start` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2024-02-01 15:31:01 nemo_logging:349] /home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:153: UserWarning: The `batch_idx` argument in `MegatronGPTSFTModel.on_train_batch_end` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "You are using a CUDA device ('NVIDIA H100 PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-02-01 15:31:01 megatron_gpt_sft_model:767] Building GPT SFT validation datasets.\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:495] Building indexing for fn = databricks-dolly-15k/validation.jsonl\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:507] Saving idx file = databricks-dolly-15k/validation.jsonl.idx.npy\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:509] Saving metadata file = databricks-dolly-15k/validation.jsonl.idx.info\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:535] Time building 1 / 1 mem-mapped files: 0:00:00.067466\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.041164\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:249] Loading databricks-dolly-15k/validation.jsonl\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000879\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-02-01 15:31:01 megatron_gpt_sft_model:770] Length of val dataset: 2251\n",
      "[NeMo I 2024-02-01 15:31:01 megatron_gpt_sft_model:774] Building GPT SFT test datasets.\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:495] Building indexing for fn = databricks-dolly-15k/test.jsonl\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:507] Saving idx file = databricks-dolly-15k/test.jsonl.idx.npy\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:509] Saving metadata file = databricks-dolly-15k/test.jsonl.idx.info\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:535] Time building 1 / 1 mem-mapped files: 0:00:00.042677\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.038185\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:249] Loading databricks-dolly-15k/test.jsonl\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000761\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-02-01 15:31:01 megatron_gpt_sft_model:777] Length of test dataset: 752\n",
      "[NeMo I 2024-02-01 15:31:01 megatron_gpt_sft_model:781] Building GPT SFT traing datasets.\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:495] Building indexing for fn = databricks-dolly-15k/training.jsonl\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:507] Saving idx file = databricks-dolly-15k/training.jsonl.idx.npy\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:509] Saving metadata file = databricks-dolly-15k/training.jsonl.idx.info\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:535] Time building 1 / 1 mem-mapped files: 0:00:00.063511\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.041611\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:249] Loading databricks-dolly-15k/training.jsonl\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000735\n",
      "[NeMo I 2024-02-01 15:31:01 text_memmap_dataset:165] Computing global indices\n",
      " > WARNING: could not find index map file databricks-dolly-15k/training.jsonl_training.jsonl_indexmap_6432mns_2046msl_0.00ssp_1234s.npy, building the indices on rank 0 ...\n",
      "[NeMo I 2024-02-01 15:31:01 dataset_utils:1303]  > building samples index mapping for training.jsonl ...\n",
      "make: Entering directory '/home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "    using uint32 for data mapping...\n",
      "    using:\n",
      "     number of documents:            12008\n",
      "     sentences range:                [0, 12008)\n",
      "     total number of sentences:      12008\n",
      "     number of epochs:               2147483646\n",
      "     maximum number of samples:      6432\n",
      "     maximum sequence length:        2046\n",
      "     short sequence probability:     0\n",
      "     short sequence ration (1/prob): 0\n",
      "     seed:                           1234\n",
      "    reached 6432 samples after 1 epochs ...\n",
      "   number of empty documents: 0\n",
      "   number of documents with one sentence: 12008\n",
      "   number of documents with long sentences: 0\n",
      "   will create mapping for 12008 samples\n",
      "[NeMo I 2024-02-01 15:31:01 dataset_utils:1324]  > done building samples index maping\n",
      "[NeMo I 2024-02-01 15:31:01 dataset_utils:1326]  > saved the index mapping in databricks-dolly-15k/training.jsonl_training.jsonl_indexmap_6432mns_2046msl_0.00ssp_1234s.npy\n",
      "[NeMo I 2024-02-01 15:31:01 dataset_utils:1328]  > elasped time to build and save samples mapping (seconds): 0.027933\n",
      "make: Entering directory '/home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "> building indices for blendable datasets ...\n",
      " > sample ratios:\n",
      "   dataset 0, input: 1, achieved: 1\n",
      "[NeMo I 2024-02-01 15:31:02 blendable_dataset:67] > elapsed time for building blendable dataset indices: 0.02 (sec)\n",
      "[NeMo I 2024-02-01 15:31:02 megatron_gpt_sft_model:783] Length of train dataset: 6432\n",
      "[NeMo I 2024-02-01 15:31:02 megatron_gpt_sft_model:788] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-02-01 15:31:02 megatron_gpt_sft_model:788] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-02-01 15:31:02 megatron_gpt_sft_model:788] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-02-01 15:31:01 nemo_logging:349] /home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/nemo/collections/nlp/data/language_modeling/megatron/dataset_utils.py:1332: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "      counts = torch.cuda.LongTensor([1])\n",
      "    \n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo W 2024-02-01 15:31:02 megatron_base_model:1119] Ignoring `trainer.max_epochs` when computing `max_steps` because `trainer.max_steps` is already set to 50.\n",
      "\n",
      "  | Name  | Type          | Params\n",
      "----------------------------------------\n",
      "0 | model | Float16Module | 2.3 B \n",
      "----------------------------------------\n",
      "6.5 M     Trainable params\n",
      "2.3 B     Non-trainable params\n",
      "2.3 B     Total params\n",
      "9,040.314 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_1\n",
      "[NeMo I 2024-02-01 15:31:02 adapter_mixins:435] Unfrozen adapter : adapter_2\n",
      "[NeMo I 2024-02-01 15:31:02 nlp_adapter_mixins:256] Optimizer groups set:\n",
      "      | Name  | Type          | Params\n",
      "    ----------------------------------------\n",
      "    0 | model | Float16Module | 2.3 B \n",
      "    ----------------------------------------\n",
      "    6.5 M     Trainable params\n",
      "    2.3 B     Non-trainable params\n",
      "    2.3 B     Total params\n",
      "    9,040.314 Total estimated model params size (MB)\n",
      "[NeMo I 2024-02-01 15:31:02 modelPT:719] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 5e-06\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2024-02-01 15:31:02 lr_scheduler:915] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f9710e49d50>\" \n",
      "    will be used during training (effective maximum steps = 50) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 50\n",
      "    )\n",
      "[NeMo I 2024-02-01 15:31:02 lr_scheduler:915] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f9710e86e60>\" \n",
      "    will be used during training (effective maximum steps = 50) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 50\n",
      "    )\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-02-01 15:31:02 nemo_logging:349] /home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 112 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2024-02-01 15:31:02 nemo_logging:349] /home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2024-02-01 15:31:02 nemo_logging:349] /home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :   0%|          | 0/50 [00:00<?]                                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-02-01 15:31:22 nemo_logging:349] /home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2024-02-01 15:31:22 nemo_logging:349] /home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('validation_loss_databricks-dolly-15k', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2024-02-01 15:31:22 nemo_logging:349] /home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('validation_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2024-02-01 15:31:22 nemo_logging:349] /home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 112 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2024-02-01 15:31:22 nemo_logging:349] /home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2024-02-01 15:31:36 nemo_logging:349] /home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2024-02-01 15:31:36 nemo_logging:349] /home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('consumed_samples', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  10%|         | 5/50 [01:18<11:48, v_num=, reduced_train_loss=2.350, global_step=4.000, consumed_samples=640.0, train_step_timing in s=15.30]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 1/18 [00:08<02:31,  8.88s/it]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 2/18 [00:18<02:30,  9.40s/it]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 3/18 [00:28<02:20,  9.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  22%|       | 4/18 [00:38<02:13,  9.53s/it]\u001b[A\n",
      "Validation DataLoader 0:  28%|       | 5/18 [00:48<02:04,  9.61s/it]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 6/18 [00:56<01:53,  9.48s/it]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 7/18 [01:06<01:44,  9.54s/it]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 8/18 [01:15<01:34,  9.45s/it]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 9/18 [01:24<01:24,  9.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 10/18 [01:33<01:15,  9.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 11/18 [01:42<01:05,  9.33s/it]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 12/18 [01:52<00:56,  9.36s/it]\u001b[A\n",
      "Validation DataLoader 0:  72%|  | 13/18 [02:01<00:46,  9.32s/it]\u001b[A\n",
      "Validation DataLoader 0:  78%|  | 14/18 [02:10<00:37,  9.29s/it]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 15/18 [02:19<00:27,  9.33s/it]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 16/18 [02:29<00:18,  9.33s/it]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 17/18 [02:39<00:09,  9.36s/it]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 18/18 [02:48<00:00,  9.37s/it]\u001b[A\n",
      "Epoch 0: :  10%|         | 5/50 [04:07<37:07, v_num=, reduced_train_loss=2.350, global_step=4.000, consumed_samples=640.0, train_step_timing in s=15.30, val_loss=2.460]\n",
      "                                                  \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 2.459\n",
      "Epoch 0, global step 5: 'validation_loss' reached 2.45895 (best 2.45895), saving model to 'results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.459-step=5-consumed_samples=640.0.ckpt' as top 1\n",
      "[NeMo W 2024-02-01 15:35:29 nemo_logging:349] /home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  14%|        | 7/50 [04:40<28:40, v_num=, reduced_train_loss=2.790, global_step=6.000, consumed_samples=896.0, train_step_timing in s=17.90, val_loss=2.460]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-02-01 15:36:02 gpt_sft_dataset:305] input is not long enough to truncate.\n",
      "[NeMo W 2024-02-01 15:36:02 gpt_sft_dataset:370] Input ids length 2304 exceed max sequence length 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  20%|        | 10/50 [05:32<22:10, v_num=, reduced_train_loss=2.600, global_step=9.000, consumed_samples=1280.0, train_step_timing in s=16.20, val_loss=2.460]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 1/18 [00:08<02:30,  8.85s/it]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 2/18 [00:18<02:31,  9.49s/it]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 3/18 [00:28<02:21,  9.43s/it]\u001b[A\n",
      "Validation DataLoader 0:  22%|       | 4/18 [00:38<02:13,  9.55s/it]\u001b[A\n",
      "Validation DataLoader 0:  28%|       | 5/18 [00:48<02:05,  9.62s/it]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 6/18 [00:56<01:53,  9.49s/it]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 7/18 [01:06<01:44,  9.54s/it]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 8/18 [01:15<01:34,  9.45s/it]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 9/18 [01:24<01:24,  9.38s/it]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 10/18 [01:33<01:15,  9.38s/it]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 11/18 [01:42<01:05,  9.32s/it]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 12/18 [01:52<00:56,  9.35s/it]\u001b[A\n",
      "Validation DataLoader 0:  72%|  | 13/18 [02:01<00:46,  9.31s/it]\u001b[A\n",
      "Validation DataLoader 0:  78%|  | 14/18 [02:09<00:37,  9.28s/it]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 15/18 [02:19<00:27,  9.32s/it]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 16/18 [02:29<00:18,  9.33s/it]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 17/18 [02:38<00:09,  9.35s/it]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 18/18 [02:48<00:00,  9.37s/it]\u001b[A\n",
      "Epoch 0: :  20%|        | 10/50 [08:21<33:24, v_num=, reduced_train_loss=2.600, global_step=9.000, consumed_samples=1280.0, train_step_timing in s=16.20, val_loss=2.440]\n",
      "                                                  \u001b[A[NeMo I 2024-02-01 15:39:43 nlp_overrides:448] Removing checkpoint: results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.459-step=5-consumed_samples=640.0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.017 >= min_delta = 0.001. New best score: 2.442\n",
      "Epoch 0, global step 10: 'validation_loss' reached 2.44224 (best 2.44224), saving model to 'results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.442-step=10-consumed_samples=1280.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-02-01 15:39:43 nlp_overrides:448] Removing checkpoint: results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.459-step=5-consumed_samples=640.0-last.ckpt\n",
      "Epoch 0: :  30%|       | 15/50 [09:40<22:34, v_num=, reduced_train_loss=2.670, global_step=14.00, consumed_samples=1920.0, train_step_timing in s=14.50, val_loss=2.440]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 1/18 [00:08<02:30,  8.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 2/18 [00:18<02:29,  9.36s/it]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 3/18 [00:28<02:20,  9.36s/it]\u001b[A\n",
      "Validation DataLoader 0:  22%|       | 4/18 [00:37<02:12,  9.49s/it]\u001b[A\n",
      "Validation DataLoader 0:  28%|       | 5/18 [00:48<02:04,  9.61s/it]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 6/18 [00:56<01:53,  9.48s/it]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 7/18 [01:06<01:44,  9.54s/it]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 8/18 [01:15<01:34,  9.44s/it]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 9/18 [01:24<01:24,  9.37s/it]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 10/18 [01:33<01:14,  9.37s/it]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 11/18 [01:42<01:05,  9.31s/it]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 12/18 [01:52<00:56,  9.34s/it]\u001b[A\n",
      "Validation DataLoader 0:  72%|  | 13/18 [02:00<00:46,  9.30s/it]\u001b[A\n",
      "Validation DataLoader 0:  78%|  | 14/18 [02:09<00:37,  9.27s/it]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 15/18 [02:19<00:27,  9.31s/it]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 16/18 [02:29<00:18,  9.31s/it]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 17/18 [02:38<00:09,  9.34s/it]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 18/18 [02:48<00:00,  9.35s/it]\u001b[A\n",
      "Epoch 0: :  30%|       | 15/50 [12:29<29:07, v_num=, reduced_train_loss=2.670, global_step=14.00, consumed_samples=1920.0, train_step_timing in s=14.50, val_loss=2.410]\n",
      "                                                  \u001b[A[NeMo I 2024-02-01 15:43:51 nlp_overrides:448] Removing checkpoint: results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.442-step=10-consumed_samples=1280.0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.030 >= min_delta = 0.001. New best score: 2.413\n",
      "Epoch 0, global step 15: 'validation_loss' reached 2.41271 (best 2.41271), saving model to 'results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.413-step=15-consumed_samples=1920.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-02-01 15:43:51 nlp_overrides:448] Removing checkpoint: results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.442-step=10-consumed_samples=1280.0-last.ckpt\n",
      "Epoch 0: :  40%|      | 20/50 [13:45<20:38, v_num=, reduced_train_loss=2.530, global_step=19.00, consumed_samples=2560.0, train_step_timing in s=18.30, val_loss=2.410]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 1/18 [00:08<02:30,  8.83s/it]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 2/18 [00:18<02:31,  9.47s/it]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 3/18 [00:28<02:21,  9.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  22%|       | 4/18 [00:38<02:13,  9.54s/it]\u001b[A\n",
      "Validation DataLoader 0:  28%|       | 5/18 [00:48<02:04,  9.62s/it]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 6/18 [00:56<01:53,  9.48s/it]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 7/18 [01:06<01:44,  9.54s/it]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 8/18 [01:15<01:34,  9.45s/it]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 9/18 [01:24<01:24,  9.38s/it]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 10/18 [01:33<01:15,  9.38s/it]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 11/18 [01:42<01:05,  9.32s/it]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 12/18 [01:52<00:56,  9.35s/it]\u001b[A\n",
      "Validation DataLoader 0:  72%|  | 13/18 [02:01<00:46,  9.31s/it]\u001b[A\n",
      "Validation DataLoader 0:  78%|  | 14/18 [02:09<00:37,  9.28s/it]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 15/18 [02:19<00:27,  9.32s/it]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 16/18 [02:29<00:18,  9.32s/it]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 17/18 [02:38<00:09,  9.35s/it]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 18/18 [02:48<00:00,  9.36s/it]\u001b[A\n",
      "Epoch 0: :  40%|      | 20/50 [16:33<24:50, v_num=, reduced_train_loss=2.530, global_step=19.00, consumed_samples=2560.0, train_step_timing in s=18.30, val_loss=2.370]\n",
      "                                                  \u001b[A[NeMo I 2024-02-01 15:47:56 nlp_overrides:448] Removing checkpoint: results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.413-step=15-consumed_samples=1920.0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.039 >= min_delta = 0.001. New best score: 2.373\n",
      "Epoch 0, global step 20: 'validation_loss' reached 2.37342 (best 2.37342), saving model to 'results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.373-step=20-consumed_samples=2560.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-02-01 15:47:56 nlp_overrides:448] Removing checkpoint: results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.413-step=15-consumed_samples=1920.0-last.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-02-01 15:47:56 gpt_sft_dataset:305] input is not long enough to truncate.\n",
      "[NeMo W 2024-02-01 15:47:56 gpt_sft_dataset:370] Input ids length 2068 exceed max sequence length 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  50%|     | 25/50 [17:57<17:57, v_num=, reduced_train_loss=2.270, global_step=24.00, consumed_samples=3200.0, train_step_timing in s=14.40, val_loss=2.370]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 1/18 [00:08<02:30,  8.85s/it]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 2/18 [00:18<02:29,  9.36s/it]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 3/18 [00:28<02:20,  9.35s/it]\u001b[A\n",
      "Validation DataLoader 0:  22%|       | 4/18 [00:38<02:13,  9.54s/it]\u001b[A\n",
      "Validation DataLoader 0:  28%|       | 5/18 [00:48<02:04,  9.61s/it]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 6/18 [00:56<01:53,  9.48s/it]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 7/18 [01:06<01:44,  9.54s/it]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 8/18 [01:15<01:34,  9.44s/it]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 9/18 [01:24<01:24,  9.38s/it]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 10/18 [01:33<01:15,  9.38s/it]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 11/18 [01:42<01:05,  9.32s/it]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 12/18 [01:52<00:56,  9.35s/it]\u001b[A\n",
      "Validation DataLoader 0:  72%|  | 13/18 [02:01<00:46,  9.31s/it]\u001b[A\n",
      "Validation DataLoader 0:  78%|  | 14/18 [02:09<00:37,  9.28s/it]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 15/18 [02:19<00:27,  9.32s/it]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 16/18 [02:29<00:18,  9.33s/it]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 17/18 [02:38<00:09,  9.35s/it]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 18/18 [02:48<00:00,  9.37s/it]\u001b[A\n",
      "Epoch 0: :  50%|     | 25/50 [20:46<20:46, v_num=, reduced_train_loss=2.270, global_step=24.00, consumed_samples=3200.0, train_step_timing in s=14.40, val_loss=2.330]\n",
      "                                                  \u001b[A[NeMo I 2024-02-01 15:52:08 nlp_overrides:448] Removing checkpoint: results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.373-step=20-consumed_samples=2560.0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.046 >= min_delta = 0.001. New best score: 2.327\n",
      "Epoch 0, global step 25: 'validation_loss' reached 2.32747 (best 2.32747), saving model to 'results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.327-step=25-consumed_samples=3200.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-02-01 15:52:08 nlp_overrides:448] Removing checkpoint: results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.373-step=20-consumed_samples=2560.0-last.ckpt\n",
      "Epoch 0: :  60%|    | 30/50 [22:11<14:47, v_num=, reduced_train_loss=2.260, global_step=29.00, consumed_samples=3840.0, train_step_timing in s=18.20, val_loss=2.330]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 1/18 [00:08<02:30,  8.85s/it]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 2/18 [00:18<02:29,  9.37s/it]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 3/18 [00:28<02:20,  9.35s/it]\u001b[A\n",
      "Validation DataLoader 0:  22%|       | 4/18 [00:37<02:12,  9.49s/it]\u001b[A\n",
      "Validation DataLoader 0:  28%|       | 5/18 [00:47<02:04,  9.57s/it]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 6/18 [00:56<01:53,  9.45s/it]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 7/18 [01:06<01:45,  9.55s/it]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 8/18 [01:15<01:34,  9.45s/it]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 9/18 [01:24<01:24,  9.38s/it]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 10/18 [01:33<01:15,  9.38s/it]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 11/18 [01:42<01:05,  9.32s/it]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 12/18 [01:52<00:56,  9.36s/it]\u001b[A\n",
      "Validation DataLoader 0:  72%|  | 13/18 [02:01<00:46,  9.31s/it]\u001b[A\n",
      "Validation DataLoader 0:  78%|  | 14/18 [02:09<00:37,  9.28s/it]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 15/18 [02:19<00:27,  9.32s/it]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 16/18 [02:29<00:18,  9.33s/it]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 17/18 [02:38<00:09,  9.35s/it]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 18/18 [02:48<00:00,  9.36s/it]\u001b[A\n",
      "Epoch 0: :  60%|    | 30/50 [24:59<16:39, v_num=, reduced_train_loss=2.260, global_step=29.00, consumed_samples=3840.0, train_step_timing in s=18.20, val_loss=2.280]\n",
      "                                                  \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.046 >= min_delta = 0.001. New best score: 2.282\n",
      "Epoch 0, global step 30: 'validation_loss' reached 2.28191 (best 2.28191), saving model to 'results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.282-step=30-consumed_samples=3840.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-02-01 15:56:22 nlp_overrides:448] Removing checkpoint: results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.327-step=25-consumed_samples=3200.0.ckpt\n",
      "[NeMo I 2024-02-01 15:56:22 nlp_overrides:448] Removing checkpoint: results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.327-step=25-consumed_samples=3200.0-last.ckpt\n",
      "Epoch 0: :  70%|   | 35/50 [26:18<11:16, v_num=, reduced_train_loss=2.310, global_step=34.00, consumed_samples=4480.0, train_step_timing in s=14.40, val_loss=2.280]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 1/18 [00:08<02:30,  8.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 2/18 [00:18<02:31,  9.48s/it]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 3/18 [00:28<02:21,  9.43s/it]\u001b[A\n",
      "Validation DataLoader 0:  22%|       | 4/18 [00:38<02:13,  9.55s/it]\u001b[A\n",
      "Validation DataLoader 0:  28%|       | 5/18 [00:48<02:04,  9.61s/it]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 6/18 [00:56<01:53,  9.48s/it]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 7/18 [01:06<01:44,  9.54s/it]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 8/18 [01:15<01:34,  9.44s/it]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 9/18 [01:24<01:24,  9.37s/it]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 10/18 [01:33<01:14,  9.37s/it]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 11/18 [01:42<01:05,  9.31s/it]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 12/18 [01:52<00:56,  9.34s/it]\u001b[A\n",
      "Validation DataLoader 0:  72%|  | 13/18 [02:00<00:46,  9.30s/it]\u001b[A\n",
      "Validation DataLoader 0:  78%|  | 14/18 [02:09<00:37,  9.27s/it]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 15/18 [02:19<00:27,  9.31s/it]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 16/18 [02:29<00:18,  9.32s/it]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 17/18 [02:38<00:09,  9.34s/it]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 18/18 [02:48<00:00,  9.35s/it]\u001b[A\n",
      "Epoch 0: :  70%|   | 35/50 [29:06<12:28, v_num=, reduced_train_loss=2.310, global_step=34.00, consumed_samples=4480.0, train_step_timing in s=14.40, val_loss=2.240]\n",
      "                                                  \u001b[A[NeMo I 2024-02-01 16:00:28 nlp_overrides:448] Removing checkpoint: results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.282-step=30-consumed_samples=3840.0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.046 >= min_delta = 0.001. New best score: 2.236\n",
      "Epoch 0, global step 35: 'validation_loss' reached 2.23581 (best 2.23581), saving model to 'results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.236-step=35-consumed_samples=4480.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-02-01 16:00:29 nlp_overrides:448] Removing checkpoint: results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.282-step=30-consumed_samples=3840.0-last.ckpt\n",
      "Epoch 0: :  80%|  | 40/50 [30:19<07:34, v_num=, reduced_train_loss=2.120, global_step=39.00, consumed_samples=5120.0, train_step_timing in s=14.40, val_loss=2.240]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 1/18 [00:08<02:30,  8.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 2/18 [00:18<02:29,  9.36s/it]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 3/18 [00:28<02:20,  9.35s/it]\u001b[A\n",
      "Validation DataLoader 0:  22%|       | 4/18 [00:37<02:12,  9.48s/it]\u001b[A\n",
      "Validation DataLoader 0:  28%|       | 5/18 [00:47<02:04,  9.56s/it]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 6/18 [00:56<01:53,  9.44s/it]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 7/18 [01:06<01:44,  9.50s/it]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 8/18 [01:15<01:34,  9.41s/it]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 9/18 [01:24<01:24,  9.38s/it]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 10/18 [01:33<01:15,  9.38s/it]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 11/18 [01:42<01:05,  9.32s/it]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 12/18 [01:52<00:56,  9.35s/it]\u001b[A\n",
      "Validation DataLoader 0:  72%|  | 13/18 [02:00<00:46,  9.31s/it]\u001b[A\n",
      "Validation DataLoader 0:  78%|  | 14/18 [02:09<00:37,  9.27s/it]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 15/18 [02:19<00:27,  9.31s/it]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 16/18 [02:29<00:18,  9.32s/it]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 17/18 [02:38<00:09,  9.34s/it]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 18/18 [02:48<00:00,  9.36s/it]\u001b[A\n",
      "Epoch 0: :  80%|  | 40/50 [33:07<08:16, v_num=, reduced_train_loss=2.120, global_step=39.00, consumed_samples=5120.0, train_step_timing in s=14.40, val_loss=2.190]\n",
      "                                                  \u001b[A[NeMo I 2024-02-01 16:04:30 nlp_overrides:448] Removing checkpoint: results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.236-step=35-consumed_samples=4480.0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.043 >= min_delta = 0.001. New best score: 2.192\n",
      "Epoch 0, global step 40: 'validation_loss' reached 2.19244 (best 2.19244), saving model to 'results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.192-step=40-consumed_samples=5120.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-02-01 16:04:30 nlp_overrides:448] Removing checkpoint: results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.236-step=35-consumed_samples=4480.0-last.ckpt\n",
      "Epoch 0: :  90%| | 45/50 [34:24<03:49, v_num=, reduced_train_loss=2.140, global_step=44.00, consumed_samples=5760.0, train_step_timing in s=14.40, val_loss=2.190]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 1/18 [00:08<02:29,  8.82s/it]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 2/18 [00:18<02:29,  9.34s/it]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 3/18 [00:27<02:19,  9.33s/it]\u001b[A\n",
      "Validation DataLoader 0:  22%|       | 4/18 [00:37<02:12,  9.47s/it]\u001b[A\n",
      "Validation DataLoader 0:  28%|       | 5/18 [00:47<02:04,  9.54s/it]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 6/18 [00:56<01:53,  9.42s/it]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 7/18 [01:06<01:44,  9.48s/it]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 8/18 [01:15<01:33,  9.39s/it]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 9/18 [01:24<01:24,  9.36s/it]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 10/18 [01:33<01:14,  9.36s/it]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 11/18 [01:42<01:05,  9.30s/it]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 12/18 [01:51<00:55,  9.33s/it]\u001b[A\n",
      "Validation DataLoader 0:  72%|  | 13/18 [02:00<00:46,  9.29s/it]\u001b[A\n",
      "Validation DataLoader 0:  78%|  | 14/18 [02:09<00:37,  9.26s/it]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 15/18 [02:19<00:27,  9.30s/it]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 16/18 [02:28<00:18,  9.31s/it]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 17/18 [02:38<00:09,  9.33s/it]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 18/18 [02:48<00:00,  9.35s/it]\u001b[A\n",
      "Epoch 0: :  90%| | 45/50 [37:13<04:08, v_num=, reduced_train_loss=2.140, global_step=44.00, consumed_samples=5760.0, train_step_timing in s=14.40, val_loss=2.150]\n",
      "                                                  \u001b[A[NeMo I 2024-02-01 16:08:35 nlp_overrides:448] Removing checkpoint: results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.192-step=40-consumed_samples=5120.0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.042 >= min_delta = 0.001. New best score: 2.151\n",
      "Epoch 0, global step 45: 'validation_loss' reached 2.15058 (best 2.15058), saving model to 'results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.151-step=45-consumed_samples=5760.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-02-01 16:08:35 nlp_overrides:448] Removing checkpoint: results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.192-step=40-consumed_samples=5120.0-last.ckpt\n",
      "Epoch 0: : 100%|| 50/50 [38:31<00:00, v_num=, reduced_train_loss=2.210, global_step=49.00, consumed_samples=6400.0, train_step_timing in s=15.80, val_loss=2.150]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 1/18 [00:08<02:30,  8.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 2/18 [00:18<02:29,  9.37s/it]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 3/18 [00:28<02:20,  9.35s/it]\u001b[A\n",
      "Validation DataLoader 0:  22%|       | 4/18 [00:37<02:12,  9.49s/it]\u001b[A\n",
      "Validation DataLoader 0:  28%|       | 5/18 [00:47<02:04,  9.57s/it]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 6/18 [00:56<01:53,  9.44s/it]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 7/18 [01:06<01:44,  9.50s/it]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 8/18 [01:15<01:34,  9.44s/it]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 9/18 [01:24<01:24,  9.37s/it]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 10/18 [01:33<01:14,  9.37s/it]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 11/18 [01:42<01:05,  9.31s/it]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 12/18 [01:52<00:56,  9.34s/it]\u001b[A\n",
      "Validation DataLoader 0:  72%|  | 13/18 [02:00<00:46,  9.30s/it]\u001b[A\n",
      "Validation DataLoader 0:  78%|  | 14/18 [02:09<00:37,  9.26s/it]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 15/18 [02:19<00:27,  9.30s/it]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 16/18 [02:29<00:18,  9.31s/it]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 17/18 [02:38<00:09,  9.33s/it]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 18/18 [02:48<00:00,  9.35s/it]\u001b[A\n",
      "Epoch 0: : 100%|| 50/50 [41:19<00:00, v_num=, reduced_train_loss=2.210, global_step=49.00, consumed_samples=6400.0, train_step_timing in s=15.80, val_loss=2.110]\n",
      "                                                  \u001b[A[NeMo I 2024-02-01 16:12:42 nlp_overrides:448] Removing checkpoint: results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.151-step=45-consumed_samples=5760.0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.044 >= min_delta = 0.001. New best score: 2.106\n",
      "Epoch 0, global step 50: 'validation_loss' reached 2.10617 (best 2.10617), saving model to 'results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.106-step=50-consumed_samples=6400.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-02-01 16:12:42 nlp_overrides:448] Removing checkpoint: results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.151-step=45-consumed_samples=5760.0-last.ckpt\n",
      "Epoch 0: : 100%|| 50/50 [41:20<00:00, v_num=, reduced_train_loss=2.210, global_step=49.00, consumed_samples=6400.0, train_step_timing in s=15.80, val_loss=2.110]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=50` reached.\n",
      "Restoring states from the checkpoint path at results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.106-step=50-consumed_samples=6400.0.ckpt\n",
      "Restored all states from the checkpoint at results/checkpoints/megatron_gpt_peft_adapter_tuning--validation_loss=2.106-step=50-consumed_samples=6400.0.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = '''\\\n",
    "dotenv -f .env run \\\n",
    "python NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "    trainer.precision=bf16 \\\n",
    "    trainer.max_steps=50 \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.val_check_interval=0.1 \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    model.restore_from_path=gpt-2b-001-hf/GPT-2B-001_bf16_tp1.nemo \\\n",
    "    model.tensor_model_parallel_size=$TP_SIZE \\\n",
    "    model.pipeline_model_parallel_size=$PP_SIZE \\\n",
    "    model.optim.lr=5e-6 \\\n",
    "    model.answer_only_loss=True \\\n",
    "    model.data.train_ds.micro_batch_size=1 \\\n",
    "    model.data.train_ds.global_batch_size=128 \\\n",
    "    model.data.train_ds.file_names=$TRAIN \\\n",
    "    model.data.train_ds.concat_sampling_probabilities=$CONCAT_SAMPLING_PROBS \\\n",
    "    model.data.validation_ds.micro_batch_size=1 \\\n",
    "    model.data.validation_ds.global_batch_size=128 \\\n",
    "    model.data.validation_ds.file_names=$VALID \\\n",
    "    model.data.validation_ds.names=$VALID_NAMES \\\n",
    "    model.data.test_ds.file_names=$TEST \\\n",
    "    model.data.test_ds.micro_batch_size=1 \\\n",
    "    model.data.test_ds.global_batch_size=256 \\\n",
    "    model.data.train_ds.num_workers=0 \\\n",
    "    model.data.validation_ds.num_workers=0 \\\n",
    "    model.data.test_ds.num_workers=0 \\\n",
    "    model.data.validation_ds.metric.name=loss \\\n",
    "    model.data.test_ds.metric.name=loss \\\n",
    "    exp_manager.create_wandb_logger=False \\\n",
    "    exp_manager.explicit_log_dir=results \\\n",
    "    exp_manager.resume_if_exists=True \\\n",
    "    exp_manager.resume_ignore_no_checkpoint=True \\\n",
    "    exp_manager.create_checkpoint_callback=True \\\n",
    "    exp_manager.checkpoint_callback_params.monitor=validation_loss\n",
    "'''\n",
    "\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-02-01 16:13:03 nemo_logging:349] /home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "[NeMo W 2024-02-01 16:13:03 nemo_logging:349] /home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/lightning_fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "      rank_zero_warn(\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_gpt_model:238] megatron-core was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/NeMo#megatron-gpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-02-01 16:13:03 megatron_gpt_generate:114] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-02-01 16:13:03 megatron_gpt_generate:115] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 20000\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 200\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.test_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: max\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: true\n",
      "        save_best_model: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 1\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: results/checkpoints/megatron_gpt_peft_adapter_tuning.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: true\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: false\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: false\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      peft:\n",
      "        peft_scheme: adapter\n",
      "        restore_from_path: null\n",
      "        restore_from_ckpt_name: null\n",
      "        restore_from_hparams_path: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "      data:\n",
      "        test_ds:\n",
      "          file_names:\n",
      "          - dolly-15k_test.jsonl\n",
      "          names:\n",
      "          - dolly-15k_test.jsonl\n",
      "          global_batch_size: 4\n",
      "          micro_batch_size: 4\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          context_key: input\n",
      "          label_key: ${data.train_ds.label_key}\n",
      "          add_eos: ${data.train_ds.add_eos}\n",
      "          add_sep: ${data.train_ds.add_sep}\n",
      "          add_bos: ${data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 20\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "    inference:\n",
      "      greedy: true\n",
      "      top_k: 0\n",
      "      top_p: 0.9\n",
      "      temperature: 1.0\n",
      "      all_probs: false\n",
      "      repetition_penalty: 1.2\n",
      "      min_tokens_to_generate: 0\n",
      "      compute_logprob: false\n",
      "      outfile_path: results/sft_results.jsonl\n",
      "      compute_attention_mask: true\n",
      "    server: false\n",
      "    port: 5555\n",
      "    web_server: false\n",
      "    share: true\n",
      "    username: test\n",
      "    password: test2\n",
      "    web_port: 9889\n",
      "    chat: false\n",
      "    chatbot_config:\n",
      "      value: false\n",
      "      attributes:\n",
      "      - name: Quality\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: quality\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Toxicity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: toxcity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Humor\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: humor\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Creativity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: creativity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Violence\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: violence\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Helpfulness\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: helpfulness\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Not_Appropriate\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: not_appropriate\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Language\n",
      "        choices:\n",
      "        - ar\n",
      "        - bg\n",
      "        - bn\n",
      "        - ca\n",
      "        - cs\n",
      "        - da\n",
      "        - de\n",
      "        - el\n",
      "        - en\n",
      "        - eo\n",
      "        - es\n",
      "        - eu\n",
      "        - fa\n",
      "        - fi\n",
      "        - fr\n",
      "        - gl\n",
      "        - he\n",
      "        - hu\n",
      "        - id\n",
      "        - it\n",
      "        - ja\n",
      "        - ko\n",
      "        - nb\n",
      "        - nl\n",
      "        - pl\n",
      "        - pt\n",
      "        - ro\n",
      "        - ru\n",
      "        - sk\n",
      "        - sv\n",
      "        - th\n",
      "        - tr\n",
      "        - uk\n",
      "        - vi\n",
      "        - zh\n",
      "        key: lang\n",
      "        type: list\n",
      "        default: en\n",
      "      user: User\n",
      "      assistant: Assistant\n",
      "      system: 'A chat between a curious human and an artificial intelligence assistant.\n",
      "        The assistant gives helpful, detailed, and polite answers to the human''s questions.\n",
      "    \n",
      "    \n",
      "        '\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "24-02-01 16:13:03 - PID:27811 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 1\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:1078] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-02-01 16:13:03 megatron_base_model:492] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-02-01 16:13:03 megatron_init:241] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-02-01 16:13:03 megatron_init:247] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-02-01 16:13:03 megatron_init:252] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-02-01 16:13:03 megatron_init:255] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-02-01 16:13:03 megatron_init:272] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-02-01 16:13:03 megatron_init:275] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-02-01 16:13:03 megatron_init:276] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-02-01 16:13:03 megatron_init:287] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-02-01 16:13:03 megatron_init:288] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-02-01 16:13:03 megatron_init:298] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-02-01 16:13:03 megatron_init:302] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-02-01 16:13:03 megatron_init:303] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-02-01 16:13:03 megatron_init:317] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-02-01 16:13:03 megatron_init:329] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-02-01 16:13:03 megatron_init:335] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-02-01 16:13:03 megatron_init:336] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-02-01 16:13:03 megatron_init:337] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-02-01 16:13:03 megatron_init:338] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2024-02-01 16:13:03 tokenizer_utils:191] Getting SentencePiece with model: /tmp/tmpp5kk9frg/2053796188904e679f7e2754a2a1f280_mt_nlg_plus_multilingual_ja_zh_the_stack_frac_015_256k.model\n",
      "[NeMo I 2024-02-01 16:13:03 megatron_base_model:520] Padded vocab_size: 256000, original vocab_size: 256000, dummy tokens: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error executing job with overrides: ['model.restore_from_path=results/checkpoints/megatron_gpt_peft_adapter_tuning.nemo', 'model.peft.restore_from_path=null', 'trainer.devices=1', 'model.data.test_ds.file_names=[dolly-15k_test.jsonl]', 'model.data.test_ds.names=[dolly-15k_test.jsonl]', 'model.data.test_ds.global_batch_size=4', 'model.data.test_ds.micro_batch_size=4', 'model.data.test_ds.tokens_to_generate=20', 'inference.greedy=True', 'inference.outfile_path=results/sft_results.jsonl']\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/quantm/NeMo_notebooks/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py\", line 123, in main\n",
      "    model = MegatronGPTSFTModel.restore_from(cfg.model.restore_from_path, model_cfg, trainer=trainer)\n",
      "  File \"/home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/nemo/collections/nlp/models/nlp_model.py\", line 465, in restore_from\n",
      "    return super().restore_from(\n",
      "  File \"/home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/nemo/core/classes/modelPT.py\", line 445, in restore_from\n",
      "    instance = cls._save_restore_connector.restore_from(\n",
      "  File \"/home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/nemo/collections/nlp/parts/nlp_overrides.py\", line 1092, in restore_from\n",
      "    super().load_instance_with_state_dict(instance, state_dict, strict)\n",
      "  File \"/home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/nemo/core/connectors/save_restore_connector.py\", line 203, in load_instance_with_state_dict\n",
      "    instance.load_state_dict(state_dict, strict=strict)\n",
      "  File \"/home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/nemo/collections/nlp/parts/mixins/nlp_adapter_mixins.py\", line 387, in load_state_dict\n",
      "    super().load_state_dict(state_dict, strict=True)\n",
      "  File \"/home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/nemo/collections/nlp/models/nlp_model.py\", line 447, in load_state_dict\n",
      "    results = super(NLPModel, self).load_state_dict(state_dict, strict=strict)\n",
      "  File \"/home/quantm/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2153, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for MegatronGPTSFTModel:\n",
      "\tMissing key(s) in state_dict: \"model.language_model.embedding.word_embeddings.weight\", \"model.language_model.rotary_pos_emb.inv_freq\", \"model.language_model.encoder.layers.0.input_layernorm.weight\", \"model.language_model.encoder.layers.0.input_layernorm.bias\", \"model.language_model.encoder.layers.0.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.0.self_attention.dense.weight\", \"model.language_model.encoder.layers.0.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.0.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.1.input_layernorm.weight\", \"model.language_model.encoder.layers.1.input_layernorm.bias\", \"model.language_model.encoder.layers.1.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.1.self_attention.dense.weight\", \"model.language_model.encoder.layers.1.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.1.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.2.input_layernorm.weight\", \"model.language_model.encoder.layers.2.input_layernorm.bias\", \"model.language_model.encoder.layers.2.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.2.self_attention.dense.weight\", \"model.language_model.encoder.layers.2.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.2.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.3.input_layernorm.weight\", \"model.language_model.encoder.layers.3.input_layernorm.bias\", \"model.language_model.encoder.layers.3.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.3.self_attention.dense.weight\", \"model.language_model.encoder.layers.3.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.3.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.4.input_layernorm.weight\", \"model.language_model.encoder.layers.4.input_layernorm.bias\", \"model.language_model.encoder.layers.4.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.4.self_attention.dense.weight\", \"model.language_model.encoder.layers.4.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.4.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.5.input_layernorm.weight\", \"model.language_model.encoder.layers.5.input_layernorm.bias\", \"model.language_model.encoder.layers.5.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.5.self_attention.dense.weight\", \"model.language_model.encoder.layers.5.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.5.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.6.input_layernorm.weight\", \"model.language_model.encoder.layers.6.input_layernorm.bias\", \"model.language_model.encoder.layers.6.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.6.self_attention.dense.weight\", \"model.language_model.encoder.layers.6.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.6.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.7.input_layernorm.weight\", \"model.language_model.encoder.layers.7.input_layernorm.bias\", \"model.language_model.encoder.layers.7.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.7.self_attention.dense.weight\", \"model.language_model.encoder.layers.7.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.7.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.8.input_layernorm.weight\", \"model.language_model.encoder.layers.8.input_layernorm.bias\", \"model.language_model.encoder.layers.8.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.8.self_attention.dense.weight\", \"model.language_model.encoder.layers.8.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.8.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.9.input_layernorm.weight\", \"model.language_model.encoder.layers.9.input_layernorm.bias\", \"model.language_model.encoder.layers.9.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.9.self_attention.dense.weight\", \"model.language_model.encoder.layers.9.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.9.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.10.input_layernorm.weight\", \"model.language_model.encoder.layers.10.input_layernorm.bias\", \"model.language_model.encoder.layers.10.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.10.self_attention.dense.weight\", \"model.language_model.encoder.layers.10.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.10.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.11.input_layernorm.weight\", \"model.language_model.encoder.layers.11.input_layernorm.bias\", \"model.language_model.encoder.layers.11.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.11.self_attention.dense.weight\", \"model.language_model.encoder.layers.11.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.11.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.12.input_layernorm.weight\", \"model.language_model.encoder.layers.12.input_layernorm.bias\", \"model.language_model.encoder.layers.12.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.12.self_attention.dense.weight\", \"model.language_model.encoder.layers.12.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.12.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.13.input_layernorm.weight\", \"model.language_model.encoder.layers.13.input_layernorm.bias\", \"model.language_model.encoder.layers.13.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.13.self_attention.dense.weight\", \"model.language_model.encoder.layers.13.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.13.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.14.input_layernorm.weight\", \"model.language_model.encoder.layers.14.input_layernorm.bias\", \"model.language_model.encoder.layers.14.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.14.self_attention.dense.weight\", \"model.language_model.encoder.layers.14.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.14.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.15.input_layernorm.weight\", \"model.language_model.encoder.layers.15.input_layernorm.bias\", \"model.language_model.encoder.layers.15.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.15.self_attention.dense.weight\", \"model.language_model.encoder.layers.15.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.15.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.16.input_layernorm.weight\", \"model.language_model.encoder.layers.16.input_layernorm.bias\", \"model.language_model.encoder.layers.16.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.16.self_attention.dense.weight\", \"model.language_model.encoder.layers.16.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.16.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.17.input_layernorm.weight\", \"model.language_model.encoder.layers.17.input_layernorm.bias\", \"model.language_model.encoder.layers.17.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.17.self_attention.dense.weight\", \"model.language_model.encoder.layers.17.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.17.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.18.input_layernorm.weight\", \"model.language_model.encoder.layers.18.input_layernorm.bias\", \"model.language_model.encoder.layers.18.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.18.self_attention.dense.weight\", \"model.language_model.encoder.layers.18.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.18.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.19.input_layernorm.weight\", \"model.language_model.encoder.layers.19.input_layernorm.bias\", \"model.language_model.encoder.layers.19.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.19.self_attention.dense.weight\", \"model.language_model.encoder.layers.19.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.19.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.20.input_layernorm.weight\", \"model.language_model.encoder.layers.20.input_layernorm.bias\", \"model.language_model.encoder.layers.20.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.20.self_attention.dense.weight\", \"model.language_model.encoder.layers.20.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.20.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.21.input_layernorm.weight\", \"model.language_model.encoder.layers.21.input_layernorm.bias\", \"model.language_model.encoder.layers.21.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.21.self_attention.dense.weight\", \"model.language_model.encoder.layers.21.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.21.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.22.input_layernorm.weight\", \"model.language_model.encoder.layers.22.input_layernorm.bias\", \"model.language_model.encoder.layers.22.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.22.self_attention.dense.weight\", \"model.language_model.encoder.layers.22.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.22.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.layers.23.input_layernorm.weight\", \"model.language_model.encoder.layers.23.input_layernorm.bias\", \"model.language_model.encoder.layers.23.self_attention.query_key_value.weight\", \"model.language_model.encoder.layers.23.self_attention.dense.weight\", \"model.language_model.encoder.layers.23.post_attention_layernorm.weight\", \"model.language_model.encoder.layers.23.post_attention_layernorm.bias\", \"model.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight\", \"model.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight\", \"model.language_model.encoder.final_layernorm.weight\", \"model.language_model.encoder.final_layernorm.bias\", \"model.language_model.output_layer.weight\". \n",
      "\tUnexpected key(s) in state_dict: \"model.language_model.encoder.layers.0.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.0.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.0.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.0.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.0.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.0.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.0.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.0.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.1.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.1.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.1.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.1.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.1.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.1.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.1.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.1.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.2.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.2.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.2.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.2.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.2.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.2.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.2.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.2.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.3.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.3.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.3.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.3.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.3.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.3.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.3.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.3.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.4.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.4.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.4.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.4.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.4.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.4.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.4.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.4.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.5.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.5.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.5.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.5.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.5.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.5.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.5.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.5.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.6.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.6.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.6.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.6.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.6.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.6.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.6.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.6.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.7.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.7.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.7.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.7.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.7.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.7.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.7.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.7.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.8.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.8.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.8.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.8.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.8.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.8.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.8.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.8.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.9.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.9.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.9.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.9.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.9.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.9.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.9.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.9.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.10.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.10.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.10.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.10.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.10.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.10.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.10.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.10.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.11.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.11.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.11.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.11.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.11.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.11.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.11.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.11.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.12.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.12.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.12.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.12.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.12.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.12.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.12.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.12.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.13.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.13.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.13.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.13.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.13.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.13.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.13.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.13.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.14.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.14.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.14.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.14.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.14.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.14.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.14.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.14.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.15.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.15.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.15.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.15.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.15.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.15.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.15.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.15.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.16.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.16.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.16.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.16.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.16.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.16.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.16.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.16.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.17.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.17.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.17.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.17.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.17.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.17.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.17.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.17.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.18.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.18.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.18.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.18.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.18.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.18.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.18.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.18.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.19.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.19.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.19.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.19.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.19.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.19.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.19.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.19.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.20.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.20.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.20.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.20.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.20.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.20.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.20.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.20.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.21.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.21.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.21.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.21.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.21.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.21.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.21.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.21.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.22.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.22.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.22.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.22.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.22.adapter_layer.adapter_1.linear_in.weight\", \"model.language_model.encoder.layers.22.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.22.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.22.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.23.adapter_layer.adapter_2.linear_out.weight\", \"model.language_model.encoder.layers.23.adapter_layer.adapter_1.layer_norm.bias\", \"model.language_model.encoder.layers.23.adapter_layer.adapter_2.layer_norm.bias\", \"model.language_model.encoder.layers.23.adapter_layer.adapter_2.linear_in.weight\", \"model.language_model.encoder.layers.23.adapter_layer.adapter_1.layer_norm.weight\", \"model.language_model.encoder.layers.23.adapter_layer.adapter_1.linear_out.weight\", \"model.language_model.encoder.layers.23.adapter_layer.adapter_2.layer_norm.weight\", \"model.language_model.encoder.layers.23.adapter_layer.adapter_1.linear_in.weight\". \n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = '''\\\n",
    "! dotenv -f .env run \\\n",
    "python NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "    model.restore_from_path=results/checkpoints/megatron_gpt_peft_adapter_tuning.nemo \\\n",
    "    model.peft.restore_from_path=null \\\n",
    "    trainer.devices=1 \\\n",
    "    model.data.test_ds.file_names=['dolly-15k_test.jsonl'] \\\n",
    "    model.data.test_ds.names=['dolly-15k_test.jsonl'] \\\n",
    "    model.data.test_ds.global_batch_size=4 \\\n",
    "    model.data.test_ds.micro_batch_size=4 \\\n",
    "    model.data.test_ds.tokens_to_generate=20 \\\n",
    "    inference.greedy=True \\\n",
    "    inference.outfile_path='results/sft_results.jsonl'  \n",
    "'''\n",
    "\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tail: cannot open 'results/sft_results.jsonl' for reading: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! tail -n 5 results/sft_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
